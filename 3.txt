HUFFMAN ENCODING
-----------------
we will use the example of 4 weathers in a city and its probabilties being 1/4 each.
so the entropy is sum(-pi.log(pi)) = 2 in this case. so now we know that to represent those four weathers, we need at least 2 bits. and this is true as
the weathers will be 00,01,10,11.

for using huffman encoding, we build a tree bottom-up:
1. write the probabilities in descending order.
2. computers use a binary system, so we start from the right by taking the 2 smallest probs. and add them and make that as its 'parent'. in case we use
   a ternary system, we select 3 smallest numbers and group them, and 4 smallest in case of 4 bit systems, and so on.
3. we keep repeating step 2 till we have reached the top of the tree with probabilty 1, as the sum should be 1.
4. now for every left traversal from top, we add a 0 and for every right one we add a 1. (in case of 3 bits, left=0, middle=1, right=2)
5. arriving at the bottom we get the representation of thse weathers in binary.

================================================
pictorial:
----------


	     	    1
     	  ___(0)____|____(1)______
     	 |		     	 |
   	1/2		    	1/2
     	 |		     	 |
 __(00)__|_(01)___	__(10)___|__(11)__
 |	 	 |	|	 	 |
1/4		1/4	1/4		1/4
================================================

in this case, all the probabilties can be represented as exact powers of two, and hence the efficiency comes out to be (entroy/avg.length) = 2/2 = 1

but if the probabilities arent exact powers of two, the efficiency reduces a bit.
ex: 1/3, 1/3, 1/9, 1/9, 1/9
entroy = 2.113
avg length = 2.22
efficiency ~ 95%

here, if we were you use 3 bit systems, then the probs are powers of 3 so the average amount of bits requred to transfer them would be lesser, and 
efficiency  would be more, saving up on costs. BUT the value of data will always be the same.
